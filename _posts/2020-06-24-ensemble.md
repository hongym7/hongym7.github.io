---
title: "Ensemble"
date: 2020-06-24 14:00:00 -0400
categories: Terms
tags: [ensemble, bagging, boosting, stacking]
---
  
#### Ensemble    
다양한 알고리즘을 사용하여 성능을 높히기 위한 방법

  
1. Voting : 각 모델의 예측을 모아서 가장 많이 선택된 클래스를 정답으로 정하는 것 
  - Voting 기법 : 각 모델의 예측을 모아서 가장 많이 선택된 클래스를 정답으로 정하는 것   
  - hard voting : 다수결 투표로 정해진 값을 따르는 것 ex) 개/고양이 분류기에서 A 입력에 대하여 4개의 모델 중 3개의 모델이 개라고 답하고 1개의 모델이 고양이라고 답했을 경우, 정답을 개로 결정하는 것 
  - soft voting : 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클랙스를 정답으로 결정하는 것    
  
2. Bagging (bootstrap aggregating) : 같은 모델을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시킴. (Voting 과의 차이점)
  - RandomForest

3. Boosting : 앞의 모델을 보완해나가면서 일련의 예측기를 학습시킴.
  - LGBM 
  - XGBoost

4. Stacking (stacked generalization) : 예측을 취합하는 간단한 함수 (아래의 voting 같은) 를 사용하는 대신 취합하는 함수를 모델화하여 훈련시킴.




  
